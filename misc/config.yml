#TODO: config hostname
# customize hostname, if omit this option, the client will use $(hostname) automatically
# hostname: "test_server1"

#TODO: split config into configuration folder
#TODO: [done 2015Mar04] normalize metric, only `a-z._` allowed in metric
#TODO: [done 2015Mar04] normalize tags
#TODO: if collect data failed, now we can provide DEFAULT value from config. how about NO-DATA alert
#   like zabbix ?
#TODO: write time precision is limited to MS (milliseconds) only
#TODO: {{.Key}}.{{.Tags}}   if Tags is empty, key will have a trailing dot.  e.g.:  `metric1.`
#TODO: 如果在进程中杀掉 hickwall, 不会自动重启，需要daemon进程监控并自动启动。 
#TODO: 开启collectors和backends，内存有溢出。

# these tags will be appended into metrics' tags field. 
tags: {
    "bu": "hotel",
    "global": "tag",
}

#TODO: enable_metadata
# only opentsdb support metadata
enable_metadata: false

# collect client performance metrics
client_metric_enabled: false
# collection interval in seconds
client_metric_interval: 1


#TODO: merge_metadata_to_tags
# If enable metadata, but backend don't support metadata. this option will merge all metadata
# into tags
merge_metadata_to_tags: false

#TODO: debug flag
debug: false

#TODO: memory_limit_in_mb
# when use 'memory' transport cache backend, we have to set a hard limit to prevent memory leak
# once this limit reached, the client will restart automatically
memory_limit_in_mb: 500

# ---------- log configurations -------------------------------------------
# only works in *nix like system
log_colored_console: true

# Levels: trace, debug, info, warn, error, critical
log_console_level: "info"

#TODO: log format instruction
# Format :
log_console_format: "%Time [%LEV] %Msg%n"

#TODO: different log path on different OS
log_file_filepath: "/var/log/hickwall/hickwall.log"
#log_file_filepath: "c:\\hickwall\\hickwall.log"

log_file_level: "debug"
log_file_format: "%Date %Time [%Level] %RelFile %Msg%n"
#log_file_format: "%Date(2006 Jan 02/3:04:05.000000000 PM MST) [%Level] %RelFile %Msg%n"

# calculated in Mb
log_file_maxsize: 100

#
log_file_maxrolls: 5

# ---------- transport configurations -------------------------------------------

#TODO: transport_flat_metric_key_format
# graphtie and influxdb version below 0.9 don't support tags.  So we need to flatten the metric key with tags
# transport flat metric key format instruction:
#  
#  {{.Key}}             is the original metric key predefiend when the collector was created, which is a string. 
#                       you can only move its position back and forth. 
#  {{.Tags}}            is the tags field of the original metric object. which is like a dict in python with key-value pairs.
#                       if a metric point have a tags lets say: { host: "dev1", bu: "hotel" }, and you place `{{.Tags}}` in
#                       the option, the client will sort tags by key and join all key and value with "_", which with the previous
#                       example data, this string will be generated: "bu_hotel_host_dev1"
#  {{.Tags_host}}       hostname, all metric will automatically add hostname into tags
#  {{.Tags_xxx}}        you can also reference field of tags directly. once a field is referenced, it will be poped out, so
#                       all fields in {tags} will only appear once in generated flat metric key.  so if you have `{{Tgas_xxx}}` 
#                       and `{tags}` in the option at the same time, then `{tags}` will only applys to the remaining tags 
# Note: {{.Lv2.Lv2}}         Multiple leveled tempalte is not allowed
transport_flat_metric_key_format: "{{.Tags_bu}}.{{.Tags_host}}.{{.Key}}.{{.Tags}}"

#TODO: config transport_backfill_enabled
# when transport target host failed, the client will try to cache up requests into a file queue. once the target host
# back online again, the client will try to send cached requests back into target host. 
# this option can turn it on/off.  if this option is off, no reqeust will be cached.
transport_backfill_enabled: false

# Try the best to report latest data to upstream is the core concept of the collector daemon. and then try to evenly
# distribute backfill workloads to prevent surge of pressure in upstream server. So two threads work seperately. 
# the first one report latest data with fixed interval, cache request if failed. 
# the second thread try to backfill data in cache with all following tweaks.

#TODO: config transport_queue_full_method
# can be 'drop' or 'trim'.  `drop` means all incoming new request will be dropped. `trim` means to push incoming request
# into queue and trim queue size to max_queue_size(oldest request will be dropped.)
transport_queue_full_method: "trim"

# can be 'memory' or 'boltq'. `boltq` is a boltdb based simple file queue, which is not optmized for performance.  
transport_queue_backend: "memory"

#TODO: config transport_queue_max_size
# max cached reqeusts count
transport_queue_max_size: 10000000

# each backfill request will delay in seconds
transport_backfill_delay: 10

# each cached request will contain multiple data points. the client will try to merge several cached requests into one
# backfill request whlie backfilling. this option decide how many of cached reqeusts can be merged into one.
# [DELETE] transport_backfill_batch: 2

# Whether the client will try to cool down a little while if a response latency threshold reached while backfilling.
transport_backfill_handsoff: false

# latency threshold in ms
transport_backfill_latency_threshold_ms: 500

# cool down period in seconds
transport_backfill_cool_down_sec: 120

# try best to merge small group of points to no more thatn max_batch_size. this option is meant to
# backfill data as quick as possible and at the same time don't add extra http request than usual.
# If you have a large cluster. it's better turn this off. coz once queue is long and backend host is
# backup online again. the network traffic will be a lot greater than normal in some cases. 
transport_merge_request: true

# graphtie transport only support plain text protocol, and will automatically flatten metric key
# transport_graphite_enabled: true
# send data to graphtie with this interval in seconds
# transport_graphite_interval: 60
# transport_graphite_hosts: ["127.0.0.1:2003", "127.0.0.1:2004"]
# transport_graphite_hosts: ["127.0.0.1:2003", "127.0.0.1:2004"]

# if influxdb version smaller than `0.9`, the client will automatically flatten metric key.
# transport_influxdb_enabled: true
# send data to influxdb with this interval in seconds
# transport_influxdb_interval: 10
# transport_influxdb_version: "0.8.8"
# transport_influxdb_hosts: ["127.0.0.1:8086", "127.0.0.1:8087"]
# transport_influxdb_database: "metrics"
# transport_influxdb_username: "root"
# transport_influxdb_password: "root"

# transport_backends:
#     - graphtie:
#         enabled: true
#         interval: 60
#         hosts: ["127.0.0.1:2003", "127.0.0.1:2004"]
#     - influxdb:
#         enabled: true
#         version: "0.8.8"
#         database: "metrics"
#         username: "root"
#         password: "root"

transport_influxdb:
    -
        version: "0.9.0-rc7"    
        enabled: false
        max_batch_size: 200
        interval_ms: 1000
        merge_reqeusts: false

        url: "http://192.168.59.103:8086/write"
        username: "root"
        password: "root"
        database: "metrics"
        retentionpolicy: "p1"

        backfill_enabled: true
        backfill_interval_s: 1
        backfill_handsoff: true
        backfill_latency_threshold_ms: 10
        backfill_cool_down_s: 5

    -
        # because v0.8.8 don't support tags, so all tags will be flatten with
        # metric key following above `transport_flat_metric_key_format`
        version: "v0.8.8"
        enabled: true
        max_batch_size: 200
        interval_ms: 1000
        merge_reqeusts: false

        host: "192.168.59.103:8086"    # v0.8.8 why to config db host
        username: "root"
        password: "root"
        database: "metrics"
        FlatTemplate: "{{.Key}}.{{.Tags}}"

        backfill_enabled: true
        backfill_interval_s: 1
        backfill_handsoff: true
        backfill_latency_threshold_ms: 10
        backfill_cool_down_s: 5

# TODO: support amqp transport
# transport_amqp_enabled: false
# transport_amqp_hosts: ["127.0.0.1:5672", "127.0.0.1:5672"]
# transport_amqp_vhost: "/"
# transport_amqp_username: "guest"
# transport_amqp_password: "guest"
# transport_amqp_exchange: "amq.fanout"
# transport_amqp_exchange_type: "fanout"
# transport_amqp_routing_key: "hickwall"
# transport_amqp_persistent: false
# transport_amqp_queue: "queuename"


# transport_kafka_enabled: true
# transport_kafka_hosts: ["127.0.0.1:2003", "127.0.0.1:2004"]

# ---------- relay configurations -------------------------------------------
# TODO: do we need to implement a rule based replay server ? or we just need a simplified relay server to go across 
# network segmentations ??

#TODO: config transport_relay_hosts
# send reqeust to relay hosts
# transport_relay_hosts: ["127.0.0.1:2345"]

# for performance sake, relay will only work in memory mode. 

# transport_relay_server: false
# transport_relay_server_upstream: ["127.0.0.1:2003", "127.0.0.1:2004"]
# transport_relay_server_rule: ""

# ---------- collector configurations -------------------------------------------

# interval in seconds
collector_default_interval: 1

# builtin collectors will always working without configurations

#TODO: metric key, tags configuration in collectors ?

#TODO: different collector type: gauge, counter 
#TODO: [delete 2015MAR03] collectored value can be transformed into different units: bytes, mb, gb, ...

collector_win_pdh:
    - 
        interval: 2
        tags: {
            "bu": "train"
        }
        queries:
            -
                query: "\\System\\Processes"
                metric: "win.pdh.process_cnt"
                # metric: "win.processes.count"     duplicated metric key: win.processes.count
            - 
                query: "\\Memory\\Available Bytes"
                metric: "win.pdh.memory.available_bytes"
            # -
            #     query: "\\Process(python)"

    - 
        interval: 2
        tags: {
            "bu": "train"
        }
        queries: 
            -
                query: "\\System\\Processes"
                metric: "win.pdh.process_cnt_1"
                tags: {
                    "mount": "C",
                    "prodution": "中文",
                }
                #TODO: support meta
                # meta: {
                #     "unit": "bytes"
                # }
            - 
                query: "\\Memory\\Available Bytes"
                metric: "win.pdh.memory.available_bytes_1"
                tags: {
                    "mount": "C"
                }

# TODO: [postponed 2015Mar05] collector_aggregation would be better to have.
# `Exits` aggregation function can apply on any kind of value.  
# But in most cases, only `int` and `float` can be aggregated. Internally, all numbers
# will be converted into float64
# TODO: graphite aggregator like aggregation ??
# collector_aggregation:
#     - 
#         # TickDriven
#         # single source and simple calc
#         Interval: 60
#         source: {
#             "A": "win.phd.memory.available_bytes"
#         }
#         aggregation: "A / 1024 / 1024"
#         metric: "win.pdh.memory.available.mb"
#         inherit_tags: "A"
#         tags: {
#             "new_tag": "aggregation"
#         }
#         # Merge(A.Tags).Merge(tags)
#         inherit_meta: "A"
#         meta: {
#             "unit": "MegaBytes",
#             "interval": "60s"
#         }

#     - 
#         # aggregated on tags
#         Interval: 60
#         source: {
#             "A": "win.pdh.memory.available.mb"
#         }

#         # because we are aggregated on tags. so tags been aggregated will not been merged
#         source_tags: 
#             - "bu"

#         aggregation: "Sum(A)"
#         metric: "win.pdh.memory.available.mb.{{.bu}}"
#         inherit_meta: "A"

#     - 
#         # TickDriven
#         # source can be generated from other aggregations
#         # single source and predefined aggregation functions with tick period
#         source: {
#             "A": "win.pdh.memory.available.mb"
#         }
#         aggregation: "Average(WindowTick(A, 5))"
#         metric: "win.pdh.memory.available.mb.avg.5tick"

#     - 
#         # single source with '*'
#         source: {
#             "A": "win.wmi.fs.*.size.bytes"
#         }

#         # for `Sum` aggregation function

#         # s1     s2     s3     s4     s5
#         # --------------------------------
#         # A1.1   A1.2   A1.3   A1.4   A1.5
#         # -      A2.1   -      A2.2   - 
#         # -      -      A3.1   -      -

#         # s1 = A1.1
#         # s2 = A1.2 + A2.1
#         # s3 = A1.3 + A2.1 + A3.1

#         # S = Last(A1) + Last(A2) + Last(A3)

#         aggregation: "Sum(A) / 1024 / 1024"
#         metric: "win.wmi.fs.total_size.mb"
#         # if drop_source == true, source metric data will not be sent to transport middleware.
#         drop_source: true

#     - 
#         # TODO: multiple source value may not arrived at the same time. 

#         # multiple source and simple calc
#         source: {
#             "A": "win.phd.memory.available_bytes",
#             "B": "win.wmi.mem.totalphysicalmemory"
#         }

#         # for `Percent` aggregation function, value will only be avaible if the last 
#         # data source have valid data tick comes in.
#         # `A` is srouce A, `B` is source B, `P` is Percent

#         # A A A A A A A 
#         # - - - - B - - 
#         # - - - - P P P

#         # TODO: [done 2015mar05]how long we have to keep data `B`, forever in stats file

#         # TODO: persist program states forever.
#         # Last Data point B will be persist forever.
#         aggregation: "Percent(A, Last(B))"  

#         metric: "win.pdh.memory.available.pct"
#         # Merge(B.Tags).Merge(A.Tags)
#         inherit_tags: "B,A" 

#     - 
#         # single source and perdefined aggregation functions with minute period

#         # TODO: aggregation on Minute Time Window
#         # Note: aggregation on Minute time window is a little bit tricky. data
#         # will not be generated once a new data point comes in. coz we may not
#         # know if the new data point is the last data tick within the unit(Minute, Day, ...)
#         #  of this time window. So we have to wait the time unit to close. then 
#         # we can calculate and emit the data.

#         # `Average(WindowMinute(A, 3))` calculation process is shown as below.
#         # m1 ... m5 represent five minutes. `[- -]` represent the minute has
#         # two time span, the first half minute, and the second half minute.
#         # `@` represent to a data tick comes in.  `|` and beneath represent
#         # a aggregation happened at that time.

#         #  m1    m2    m3    m4    m5
#         # [- -] [- -] [- -] [- -] [- -]
#         #  @     @       @
#         #                    |
#         #                    Average(m1, m2, m3)
#         #                    @ 
#         #                          |
#         #                          Average(m2, m3, m4)     
#         #                            @
#         # [- -] [- -] [- -] [- -] [- -]

#         source: {
#             "A": "win.pdh.memory.available.mb"
#         }
#         aggregation: "Average(WindowMinute(A, 5))"
#         metric: "win.pdh.memory.available.mb.avg.5min"

#     -
#         # single source, exists
#         source: {
#             "A": "win.wmi.service.iis"
#         }
#         Interval: 60
#         aggregation: "Exists(Last(A))"
#         aggregation: "Exists(Last(WindowTick(A, 100)))"
#         aggregation: "Exists(Last(WindowHour(A, 24)))"
#         metric: "win.wmi.service.iis.installed"

#     - 
#         # TickDriven
#         # single source and delta
#         source: {
#             "A": "win.phd.app.visits"
#         }
#         # delta can only applied on Window.size == 2
#         aggregation: "Delta(WindowTick(A,2))"
#         metric: "win.pdh.app.visits.delta.tick"
#     - 
#         # TickDriven
#         # single source and delta
#         source: {
#             "A": "win.phd.app.visits"
#         }
#         # delta can only applied on Window.size == 2
#         aggregation: "Delta(WindowMinute(A,2))"
#         metric: "win.pdh.app.visits.delta.minute"

#     - 
#         # single source, delta
#         source: {
#             "A": "win.pdh.app.visits.delta.minute"
#         }
#         aggregation: "Average(WindowMinute(A, 5))"
#         metric: "win.pdh.app.visits.delta.avg.5m"

#         # if drop_source == true, source metric data will not be sent to transport middleware
#         drop_source: true



# This collector is supplement for win_pdh. and is not performace optimized. so should be used with 
# limitations. Internally, all queries with in this collector will be executed sequencially.
collector_win_wmi:
    - 
        interval: 2
        tags: {
            "bu": "train",
            "prodution": "中文"
        }

        queries: 
            - 
                # query: "select * from Win32_SystemServices where Name='W3svc'"
                query: "select * from Win32_Service where Name='W3svc'"
                metrics:
                    # character cases matters here!!  
                    -
                        value_from: "State"
                        metric: "win.wmi.service.iis.state"
                        default: "IIS Not Installed"


            # - 
            #     # query: "select * from Win32_SystemServices where Name='W3svc'"
            #     query: "select * from Win32_Service where Name='W3svc'"
            #     metrics:
            #         # character cases matters here!!  
            #         -
            #             # default value
            #             value_from: "State"
            #             metric: "win.wmi.service.iis.state"
            #             default: "IIS Not Installed"


            # simplest query form. for single instance return query
            # - 
            #     # query: "wmic cpu get Name,NumberOfCores"
            #     query: "select Name, NumberOfCores from Win32_Processor"
            #     metrics:
            #         # character cases matters here!!  
            #         -
            #             value_from: "Name"
            #             metric: "win.wmi.cpu.name"
            #         -
            #             value_from: "NumberOfCores"
            #             metric: "win.wmi.cpu.numberofcores"

            # # query with metric templating, tags for multiple instance return query
            # - 
            #     # query: "wmic logicaldisk get Name, FileSystem, FreeSpace"
            #     # query: "wmic logicaldisk where 'mediatype=11 or mediatype=12' get Name, FileSystem, FreeSpace, Size"
            #     query: "select Name, FileSystem, FreeSpace, Size from Win32_LogicalDisk where MediaType=11 or mediatype=12"

            #     #  map[FreeSpace: Name:A: FileSystem:]
            #     #  map[Name:C: FileSystem:NTFS FreeSpace:57517752320]
            #     #  map[FileSystem:CDFS FreeSpace:0 Name:D:]

            #     tags: {
            #         "tag_level": "query_tag"
            #     }
            #     # metric string is a template. {Name} means use the value of `Name` Field of collected record. 
            #     metrics:
            #         # character cases matters here!!  
            #         -
            #             value_from: "Size"
            #             metric: "win.wmi.fs.{{.Name}}.{{.FileSystem}}.size.bytes"
            #             tags: {
            #                 "mount": "{{.Name}}",
            #                 "fs_type": "{{.FileSystem}}",
            #             }
            #         -
            #             value_from: "FreeSpace"
            #             metric: "win.wmi.fs.{{.Name}}.{{.FileSystem}}.freespace.bytes"
            #             tags: {
            #                 "mount": "{{.Name}}",
            #                 "fs_type": "{{.FileSystem}}",
            #             }
            #             #TODO: support meta
            #             # meta: {
            #             #     "unit": "bytes"
            #             # }
            #         - 
            #             value_from: "FileSystem"
            #             metric: "win.wmi.fs.{{.Name}}.filesystem"

# TODO: collector_mysql_query
# collector_mysql_query:
#     - 
#         tags: [
#             [ "bu", "test" ],
#         ]
#         host     : "127.0.0.1"
#         port     : 3306
#         username : "root"
#         password : "root"

#         queries  :
#             -
#                 metric_key: "mysql_query.xxxxx"
#                 # tags: [
#                 #     [ "some", "test2"]
#                 # ]

#                 database: "db1"
#                 desc: "one line query"
#                 query: "SELECT count(*) as cnt, sum(*) as total FROM sometable where column=123"
#                 values_from: "cnt"

#             # metrics:
#             #   mysql_query.xxxx  {"bu":"test", "some", "test2"}      123

#             -
#                 database: "db1"
#                 desc: "multiple line query"
#                 # multiple line is tricky. indent is very important
#                 #
#                 #  xxxx: >
#                 #      a          =>  "a b\n"
#                 #      b
#                 #  xxxx: >
#                 #      a          =>  "a\n b\n"
#                 #       b
#                 query: >
#                     SELECT count(*) as cnt, sum(*) as
#                     total FROM sometable where column=123

#                 metric_key: "mysql_query.xxxxx"

#                 # multiple values from also ok
#                 values_from: "cnt, total"

#             # metrics:
#             #   mysql_query.xxxx.cnt    {"bu": "test"}  123
#             #   mysql_query.xxxx.total  {"bu": "test"}  432

# TODO: collector_ping
# collector_ping:
#     -
#         # in seconds
#         interval: 10
#         metric_key: "ping"
#         tags: [
#             [ "some", "test2"]
#         ]
#         hosts:
#             - "www.baidu.com"
#             - "www.12306.com"
#         timeout_ms: 1000ms
#         packets: 5
#         collect:
#             - "time_min"
#             - "time_avg"
#             - "time_max"
#             - "time_mdev"
#             - "lost_pct"

#         # metrics:
#         #   ping.time_min    {"some": "test2", "host": "www.baidu.com"}  28.307
#         #   ping.time_avg    {"some": "test2", "host": "www.baidu.com"}  30.372
#         #   ping.time_max    {"some": "test2", "host": "www.baidu.com"}  34.360
#         #   ping.time_mdev   {"some": "test2", "host": "www.baidu.com"}  2.192
#         #   ping.lost_pct    {"some": "test2", "host": "www.baidu.com"}  0.0

#         #   ping.time_min    {"some": "test2", "host": "www.12306.com"}  28.307
#         #   ping.time_avg    {"some": "test2", "host": "www.12306.com"}  30.372
#         #   ping.time_max    {"some": "test2", "host": "www.12306.com"}  34.360
#         #   ping.time_mdev   {"some": "test2", "host": "www.12306.com"}  2.192
#         #   ping.lost_pct    {"some": "test2", "host": "www.12306.com"}  0.0


# collecotr_linux:
# collector_darwin:

# plugin_cmd:
#     - 
#         alias: "cmd1"
#         cmd: "/bin/echo"
#         regex: "/"

# plugin_cmd:
#     -
#         alias: "python"
#         cmd: "/bin/python /path/to/py/script.py"
#         regex: "\d+"







